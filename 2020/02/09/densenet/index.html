<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>DenseNet神经网络结构 | 花花的blog | 厚积薄发</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="Deep Learning,神经网络,DenseNet">
    <meta name="description" content="CVPR 2017最佳论文DenseNet，论文中提出的DenseNet（Dense Convolutional Network） 。">
<meta name="keywords" content="Deep Learning,神经网络,DenseNet">
<meta property="og:type" content="article">
<meta property="og:title" content="DenseNet神经网络结构">
<meta property="og:url" content="https://1187100546.github.io/2020/02/09/densenet/index.html">
<meta property="og:site_name" content="花花的blog">
<meta property="og:description" content="CVPR 2017最佳论文DenseNet，论文中提出的DenseNet（Dense Convolutional Network） 。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7BL%28L%2B1%29%7D%7B2%7D">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/09/1foPQe.png">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/09/1foX6g.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=l">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5C%5Cx_l+%3D+H_l%28x_%7Bl-1%7D%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5C%5Cx_l+%3D+H_l%28x_%7Bl-1%7D%29+%2B+x_%7Bl-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5C%5Cx_l+%3D+H_l%28%5Bx_0%2C+x_1%2C+...%2C+x_%7Bl-1%7D%5D%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=l-1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_2">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-0b28a49f274da9bd8dec2dccddf1ec53_hd.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-7114e1583347944ea287b9552202b0ab_hd.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=4k">
<meta property="og:image" content="https://pic1.zhimg.com/v2-4de2c07f516b030b864181c79734dc8c_r.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctheta%3D0.5">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%7BL%3D40%2C+k%3D12%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=2k">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-c712b7a04200ecfd05c79478adb18888_hd.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/09/1fbO5n.png">
<meta property="og:updated_time" content="2020-02-09T07:49:23.481Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DenseNet神经网络结构">
<meta name="twitter:description" content="CVPR 2017最佳论文DenseNet，论文中提出的DenseNet（Dense Convolutional Network） 。">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7BL%28L%2B1%29%7D%7B2%7D">
    
        <link rel="alternate" type="application/atom+xml" title="花花的blog" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/hua.jpg">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <link rel="stylesheet" href="/css/prism/prism-tomorrow-night.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-list-ul"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/OIP.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">huahua</h5>
          <a href="mailto:1187100546@qq.com" title="1187100546@qq.com" class="mail">1187100546@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives/"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories/"  >
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags/"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/1187100546"  >
                <i class="icon icon-lg icon-github"></i>
                github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about/"  >
                <i class="icon icon-lg icon-info-circle"></i>
                关于
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">DenseNet神经网络结构</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">DenseNet神经网络结构</h1>
        <h5 class="subtitle">
            
                <time datetime="2020-02-09T05:52:33.000Z" itemprop="datePublished" class="page-time">
  2020-02-09
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/深度学习/">深度学习</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#优点"><span class="post-toc-number">1.</span> <span class="post-toc-text">优点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#设计理念"><span class="post-toc-number">2.</span> <span class="post-toc-text">设计理念</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#网络结构"><span class="post-toc-number">3.</span> <span class="post-toc-text">网络结构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#代码实现"><span class="post-toc-number">4.</span> <span class="post-toc-text">代码实现</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#一维代码"><span class="post-toc-number">5.</span> <span class="post-toc-text">一维代码</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#完整代码"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">完整代码</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#运行结果"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">运行结果</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-densenet"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">DenseNet神经网络结构</h1>
        <div class="post-meta">
            <time class="post-time" title="2020-02-09 13:52:33" datetime="2020-02-09T05:52:33.000Z"  itemprop="datePublished">2020-02-09</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/深度学习/">深度学习</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


            
        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>CVPR 2017最佳论文DenseNet，论文中提出的DenseNet（Dense Convolutional Network） 。</p>
<a id="more"></a>
<p>作为CVPR2017年的Best Paper, DenseNet脱离了加深网络层数(ResNet)和加宽网络结构(Inception)来提升网络性能的定式思维,从特征的角度考虑,通过特征重用和旁路(Bypass)设置,既大幅度减少了网络的参数量,又在一定程度上缓解了gradient vanishing（梯度消失）问题的产生.结合信息流和特征复用的假设,DenseNet当之无愧成为2017年计算机视觉顶会的年度最佳论文.</p>
<p>随着CNN网络层数的不断增加,gradient vanishing和model degradation（模型退化）问题出现在了人们面前,BatchNormalization（批规范化）的广泛使用在一定程度上缓解了gradient vanishing的问题,而ResNet和Highway Networks通过构造恒等映射设置旁路,进一步减少了gradient vanishing和model degradation的产生.Fractal Nets通过将不同深度的网络并行化,在获得了深度的同时保证了梯度的传播,随机深度网络通过对网络中一些层进行失活,既证明了ResNet深度的冗余性,又缓解了上述问题的产生. 虽然这些不同的网络框架通过不同的实现加深的网络层数,但是他们都包含了相同的核心思想,既将feature map进行跨网络层的连接.</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li><strong>减轻了vanishing-gradient（梯度消失）</strong></li>
<li><strong>加强了feature的传递</strong> </li>
<li><strong>更有效地利用了feature</strong> </li>
<li><strong>一定程度上较少了参数数量</strong> </li>
</ul>
<h2 id="设计理念"><a href="#设计理念" class="headerlink" title="设计理念"></a>设计理念</h2><p>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。图1为ResNet网络的连接机制，作为对比，图2为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个 L层的网络，DenseNet共包含 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BL%28L%2B1%29%7D%7B2%7D" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。 </p>
<p><img src="https://s2.ax1x.com/2020/02/09/1foPQe.png" alt="1foPQe.png"></p>
<p><center> 图1 ResNet网络的短路连接机制（其中+代表的是元素级相加操作） </center><br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2020/02/09/1foX6g.png" alt="1foX6g.png" title>
                </div>
                <div class="image-caption">1foX6g.png</div>
            </figure></p>
<p><center> 图2 DenseNet网络的密集连接机制（其中c代表的是channel级连接操作）</center><br> 如果用公式表示的话，传统的网络在 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=l" alt="[公式]" title>
                </div>
                <div class="image-caption">[公式]</div>
            </figure> 层的输出为：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=%5C%5Cx_l+%3D+H_l%28x_%7Bl-1%7D%29" alt title>
                </div>
                <div class="image-caption"></div>
            </figure> </p>
<p>而对于ResNet，增加了来自上一层输入的identity函数：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=%5C%5Cx_l+%3D+H_l%28x_%7Bl-1%7D%29+%2B+x_%7Bl-1%7D" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>在DenseNet中，会连接前面所有层作为输入：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=%5C%5Cx_l+%3D+H_l%28%5Bx_0%2C+x_1%2C+...%2C+x_%7Bl-1%7D%5D%29" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>其中，上面的 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=l-1" alt="](https://www.zhihu.com/equation?tex=H_l%28%5Ccdot%29) 代表是非线性转化函数（non-liear transformation），它是一个组合操作，其可能包括一系列的BN(Batch Normalization)，ReLU，Pooling及Conv操作。注意这里 ![[公式]](https://www.zhihu.com/equation?tex=l) 层与 ![[公式]" title>
                </div>
                <div class="image-caption">](https://www.zhihu.com/equation?tex=H_l%28%5Ccdot%29) 代表是非线性转化函数（non-liear transformation），它是一个组合操作，其可能包括一系列的BN(Batch Normalization)，ReLU，Pooling及Conv操作。注意这里 ![[公式]](https://www.zhihu.com/equation?tex=l) 层与 ![[公式]</div>
            </figure> 层之间可能实际上包含多个卷积层。</p>
<p>DenseNet的前向过程如图3所示，可以更直观地理解其密集连接方式，比如 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=x_2" alt="[公式]](https://www.zhihu.com/equation?tex=h_3) 的输入不仅包括来自 ![[公式]](https://www.zhihu.com/equation?tex=h_2) 的 ![[公式]](https://www.zhihu.com/equation?tex=x_2) ，还包括前面两层的 ![[公式]](https://www.zhihu.com/equation?tex=x_1) 和 ![[公式]" title>
                </div>
                <div class="image-caption">[公式]](https://www.zhihu.com/equation?tex=h_3) 的输入不仅包括来自 ![[公式]](https://www.zhihu.com/equation?tex=h_2) 的 ![[公式]](https://www.zhihu.com/equation?tex=x_2) ，还包括前面两层的 ![[公式]](https://www.zhihu.com/equation?tex=x_1) 和 ![[公式]</div>
            </figure> ，它们是在channel维度上连接在一起的。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p> 如图所示，DenseNet的网络结构主要由DenseBlock和Transition组成。 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://pic4.zhimg.com/80/v2-0b28a49f274da9bd8dec2dccddf1ec53_hd.jpg" alt="img" title>
                </div>
                <div class="image-caption">img</div>
            </figure> </p>
<p>在DenseBlock中，各个层的特征图大小一致，可以在channel维度上连接。DenseBlock中的非线性组合函数 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]](https://www.zhihu.com/equation?tex=H%28%5Ccdot%29) 采用的是**BN+ReLU+3x3 Conv**的结构，如图所示。另外值得注意的一点是，与ResNet不同，所有DenseBlock中各个层卷积之后均输出 ![[公式]](https://www.zhihu.com/equation?tex=k) 个特征图，即得到的特征图的channel数为 ![[公式]](https://www.zhihu.com/equation?tex=k) ，或者说采用 ![[公式]](https://www.zhihu.com/equation?tex=k) 个卷积核。 ![[公式]](https://www.zhihu.com/equation?tex=k) 在DenseNet称为growth rate，这是一个超参数。一般情况下使用较小的 ![[公式]](https://www.zhihu.com/equation?tex=k) （比如12），就可以得到较佳的性能。假定输入层的特征图的channel数为 ![[公式]](https://www.zhihu.com/equation?tex=k_0) ，那么 ![[公式]](https://www.zhihu.com/equation?tex=l) 层输入的channel数为 ![[公式]](https://www.zhihu.com/equation?tex=k_0%2Bk%28l-1%29) ，因此随着层数增加，尽管 ![[公式]](https://www.zhihu.com/equation?tex=k) 设定得较小，DenseBlock的输入会非常多，不过这是由于特征重用所造成的，每个层仅有 ![[公式]" title>
                </div>
                <div class="image-caption">[公式]](https://www.zhihu.com/equation?tex=H%28%5Ccdot%29) 采用的是**BN+ReLU+3x3 Conv**的结构，如图所示。另外值得注意的一点是，与ResNet不同，所有DenseBlock中各个层卷积之后均输出 ![[公式]](https://www.zhihu.com/equation?tex=k) 个特征图，即得到的特征图的channel数为 ![[公式]](https://www.zhihu.com/equation?tex=k) ，或者说采用 ![[公式]](https://www.zhihu.com/equation?tex=k) 个卷积核。 ![[公式]](https://www.zhihu.com/equation?tex=k) 在DenseNet称为growth rate，这是一个超参数。一般情况下使用较小的 ![[公式]](https://www.zhihu.com/equation?tex=k) （比如12），就可以得到较佳的性能。假定输入层的特征图的channel数为 ![[公式]](https://www.zhihu.com/equation?tex=k_0) ，那么 ![[公式]](https://www.zhihu.com/equation?tex=l) 层输入的channel数为 ![[公式]](https://www.zhihu.com/equation?tex=k_0%2Bk%28l-1%29) ，因此随着层数增加，尽管 ![[公式]](https://www.zhihu.com/equation?tex=k) 设定得较小，DenseBlock的输入会非常多，不过这是由于特征重用所造成的，每个层仅有 ![[公式]</div>
            </figure> 个特征是自己独有的。</p>
<p> <img src="https://pic4.zhimg.com/80/v2-7114e1583347944ea287b9552202b0ab_hd.jpg" alt="img"> </p>
<p>由于后面层的输入会非常大，DenseBlock内部可以采用bottleneck层来减少计算量，主要是原有的结构中增加1x1 Conv，如图所示，即<strong>BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv</strong>，称为DenseNet-B结构。其中1x1 Conv得到 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=4k" alt="[公式]" title>
                </div>
                <div class="image-caption">[公式]</div>
            </figure> 个特征图它起到的作用是降低特征数量，从而提升计算效率。 </p>
<p><img src="https://pic1.zhimg.com/v2-4de2c07f516b030b864181c79734dc8c_r.jpg" alt="preview"> </p>
<p>对于Transition层，它主要是连接两个相邻的DenseBlock，并且降低特征图大小。Transition层包括一个1x1的卷积和2x2的AvgPooling，结构为<strong>BN+ReLU+1x1 Conv+2x2 AvgPooling</strong>。另外，Transition层可以起到压缩模型的作用。假定Transition的上接DenseBlock得到的特征图channels数为 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=%5Ctheta%3D0.5" alt="[公式]](https://www.zhihu.com/equation?tex=m) ，Transition层可以产生 ![[公式]](https://www.zhihu.com/equation?tex=%5Clfloor%5Ctheta+m%5Crfloor) 个特征（通过卷积层），其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cin+%280%2C1%5D) 是压缩系数（compression rate）。当 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta%3D1) 时，特征个数经过Transition层没有变化，即无压缩，而当压缩系数小于1时，这种结构称为DenseNet-C，文中使用 ![[公式]" title>
                </div>
                <div class="image-caption">[公式]](https://www.zhihu.com/equation?tex=m) ，Transition层可以产生 ![[公式]](https://www.zhihu.com/equation?tex=%5Clfloor%5Ctheta+m%5Crfloor) 个特征（通过卷积层），其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cin+%280%2C1%5D) 是压缩系数（compression rate）。当 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta%3D1) 时，特征个数经过Transition层没有变化，即无压缩，而当压缩系数小于1时，这种结构称为DenseNet-C，文中使用 ![[公式]</div>
            </figure> 。对于使用bottleneck层的DenseBlock结构和压缩系数小于1的Transition组合结构称为DenseNet-BC。</p>
<p>DenseNet共在三个图像分类数据集（CIFAR，SVHN和ImageNet）上进行测试。对于前两个数据集，其输入图片大小为 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=%7BL%3D40%2C+k%3D12%7D" alt="[公式]](https://www.zhihu.com/equation?tex=32%5Ctimes+32) ，所使用的DenseNet在进入第一个DenseBlock之前，首先进行进行一次3x3卷积（stride=1），卷积核数为16（对于DenseNet-BC为 ![[公式]](https://www.zhihu.com/equation?tex=2k) ）。DenseNet共包含三个DenseBlock，各个模块的特征图大小分别为 ![[公式]](https://www.zhihu.com/equation?tex=32%5Ctimes+32) ， ![[公式]](https://www.zhihu.com/equation?tex=16%5Ctimes+16) 和 ![[公式]](https://www.zhihu.com/equation?tex=8%5Ctimes+8) ，每个DenseBlock里面的层数相同。最后的DenseBlock之后是一个global AvgPooling层，然后送入一个softmax分类器。注意，在DenseNet中，所有的3x3卷积均采用padding=1的方式以保证特征图大小维持不变。对于基本的DenseNet，使用如下三种网络配置： ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D40%2C+k%3D12%5C%7D) , ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D100%2C+k%3D12%5C%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D40%2C+k%3D24%5C%7D) 。而对于DenseNet-BC结构，使用如下三种网络配置： ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D100%2C+k%3D12%5C%7D) , ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D250%2C+k%3D24%5C%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D190%2C+k%3D40%5C%7D) 。这里的 ![[公式]](https://www.zhihu.com/equation?tex=L) 指的是网络总层数（网络深度），一般情况下，我们只把带有训练参数的层算入其中，而像Pooling这样的无参数层不纳入统计中，此外BN层尽管包含参数但是也不单独统计，而是可以计入它所附属的卷积层。对于普通的 ![[公式]" title>
                </div>
                <div class="image-caption">[公式]](https://www.zhihu.com/equation?tex=32%5Ctimes+32) ，所使用的DenseNet在进入第一个DenseBlock之前，首先进行进行一次3x3卷积（stride=1），卷积核数为16（对于DenseNet-BC为 ![[公式]](https://www.zhihu.com/equation?tex=2k) ）。DenseNet共包含三个DenseBlock，各个模块的特征图大小分别为 ![[公式]](https://www.zhihu.com/equation?tex=32%5Ctimes+32) ， ![[公式]](https://www.zhihu.com/equation?tex=16%5Ctimes+16) 和 ![[公式]](https://www.zhihu.com/equation?tex=8%5Ctimes+8) ，每个DenseBlock里面的层数相同。最后的DenseBlock之后是一个global AvgPooling层，然后送入一个softmax分类器。注意，在DenseNet中，所有的3x3卷积均采用padding=1的方式以保证特征图大小维持不变。对于基本的DenseNet，使用如下三种网络配置： ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D40%2C+k%3D12%5C%7D) , ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D100%2C+k%3D12%5C%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D40%2C+k%3D24%5C%7D) 。而对于DenseNet-BC结构，使用如下三种网络配置： ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D100%2C+k%3D12%5C%7D) , ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D250%2C+k%3D24%5C%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BL%3D190%2C+k%3D40%5C%7D) 。这里的 ![[公式]](https://www.zhihu.com/equation?tex=L) 指的是网络总层数（网络深度），一般情况下，我们只把带有训练参数的层算入其中，而像Pooling这样的无参数层不纳入统计中，此外BN层尽管包含参数但是也不单独统计，而是可以计入它所附属的卷积层。对于普通的 ![[公式]</div>
            </figure> 网络，除去第一个卷积层、2个Transition中卷积层以及最后的Linear层，共剩余36层，均分到三个DenseBlock可知每个DenseBlock包含12层。其它的网络配置同样可以算出各个DenseBlock所含层数。</p>
<p>对于ImageNet数据集，图片输入大小为 <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=2k" alt="[公式]](https://www.zhihu.com/equation?tex=224%5Ctimes+224) ，网络结构采用包含4个DenseBlock的DenseNet-BC，其首先是一个stride=2的7x7卷积层（卷积核数为 ![[公式]" title>
                </div>
                <div class="image-caption">[公式]](https://www.zhihu.com/equation?tex=224%5Ctimes+224) ，网络结构采用包含4个DenseBlock的DenseNet-BC，其首先是一个stride=2的7x7卷积层（卷积核数为 ![[公式]</div>
            </figure> ），然后是一个stride=2的3x3 MaxPooling层，后面才进入DenseBlock。ImageNet数据集所采用的网络配置如表所示：</p>
<p> <img src="https://pic1.zhimg.com/80/v2-c712b7a04200ecfd05c79478adb18888_hd.jpg" alt="img"> </p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><pre><code class="lang-python">import tensorflow as tf
from tflearn.layers.conv import global_avg_pool
from tensorflow.contrib.layers import batch_norm, flatten
from tensorflow.contrib.layers import xavier_initializer
from tensorflow.contrib.framework import arg_scope
from cifar10 import *

# Hyperparameter
growth_k = 24
nb_block = 2 # how many (dense block + Transition Layer) ?
init_learning_rate = 1e-4
epsilon = 1e-4 # AdamOptimizer epsilon
dropout_rate = 0.2

# Momentum Optimizer will use
nesterov_momentum = 0.9
weight_decay = 1e-4

# Label &amp; batch_size
batch_size = 64

iteration = 782
# batch_size * iteration = data_set_number

test_iteration = 10

total_epochs = 300

def conv_layer(input, filter, kernel, stride=1, layer_name=&quot;conv&quot;):
    with tf.name_scope(layer_name):
        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding=&#39;SAME&#39;)
        return network

def Global_Average_Pooling(x, stride=1):
    &quot;&quot;&quot;
    width = np.shape(x)[1]
    height = np.shape(x)[2]
    pool_size = [width, height]
    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride) # The stride value does not matter
    It is global average pooling without tflearn
    &quot;&quot;&quot;

    return global_avg_pool(x, name=&#39;Global_avg_pooling&#39;)
    # But maybe you need to install h5py and curses or not


def Batch_Normalization(x, training, scope):
    with arg_scope([batch_norm],
                   scope=scope,
                   updates_collections=None,
                   decay=0.9,
                   center=True,
                   scale=True,
                   zero_debias_moving_mean=True) :
        return tf.cond(training,
                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),
                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))

def Drop_out(x, rate, training) :
    return tf.layers.dropout(inputs=x, rate=rate, training=training)

def Relu(x):
    return tf.nn.relu(x)

def Average_pooling(x, pool_size=[2,2], stride=2, padding=&#39;VALID&#39;):
    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)


def Max_Pooling(x, pool_size=[3,3], stride=2, padding=&#39;VALID&#39;):
    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)

def Concatenation(layers) :
    return tf.concat(layers, axis=3)

def Linear(x) :
    return tf.layers.dense(inputs=x, units=class_num, name=&#39;linear&#39;)

def Evaluate(sess):
    test_acc = 0.0
    test_loss = 0.0
    test_pre_index = 0
    add = 1000

    for it in range(test_iteration):
        test_batch_x = test_x[test_pre_index: test_pre_index + add]
        test_batch_y = test_y[test_pre_index: test_pre_index + add]
        test_pre_index = test_pre_index + add

        test_feed_dict = {
            x: test_batch_x,
            label: test_batch_y,
            learning_rate: epoch_learning_rate,
            training_flag: False
        }

        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)

        test_loss += loss_ / 10.0
        test_acc += acc_ / 10.0

    summary = tf.Summary(value=[tf.Summary.Value(tag=&#39;test_loss&#39;, simple_value=test_loss),
                                tf.Summary.Value(tag=&#39;test_accuracy&#39;, simple_value=test_acc)])

    return test_acc, test_loss, summary

class DenseNet():
    def __init__(self, x, nb_blocks, filters, training):
        self.nb_blocks = nb_blocks
        self.filters = filters
        self.training = training
        self.model = self.Dense_net(x)


    def bottleneck_layer(self, x, scope):
        # print(x)
        with tf.name_scope(scope):
            x = Batch_Normalization(x, training=self.training, scope=scope+&#39;_batch1&#39;)
            x = Relu(x)
            x = conv_layer(x, filter=4 * self.filters, kernel=[1,1], layer_name=scope+&#39;_conv1&#39;)
            x = Drop_out(x, rate=dropout_rate, training=self.training)

            x = Batch_Normalization(x, training=self.training, scope=scope+&#39;_batch2&#39;)
            x = Relu(x)
            x = conv_layer(x, filter=self.filters, kernel=[3,3], layer_name=scope+&#39;_conv2&#39;)
            x = Drop_out(x, rate=dropout_rate, training=self.training)

            # print(x)

            return x

    def transition_layer(self, x, scope):
        with tf.name_scope(scope):
            x = Batch_Normalization(x, training=self.training, scope=scope+&#39;_batch1&#39;)
            x = Relu(x)
            # x = conv_layer(x, filter=self.filters, kernel=[1,1], layer_name=scope+&#39;_conv1&#39;)

            # https://github.com/taki0112/Densenet-Tensorflow/issues/10

            in_channel = x.shape[-1]
            x = conv_layer(x, filter=in_channel*0.5, kernel=[1,1], layer_name=scope+&#39;_conv1&#39;)
            x = Drop_out(x, rate=dropout_rate, training=self.training)
            x = Average_pooling(x, pool_size=[2,2], stride=2)

            return x

    def dense_block(self, input_x, nb_layers, layer_name):
        with tf.name_scope(layer_name):
            layers_concat = list()
            layers_concat.append(input_x)

            x = self.bottleneck_layer(input_x, scope=layer_name + &#39;_bottleN_&#39; + str(0))

            layers_concat.append(x)

            for i in range(nb_layers - 1):
                x = Concatenation(layers_concat)
                x = self.bottleneck_layer(x, scope=layer_name + &#39;_bottleN_&#39; + str(i + 1))
                layers_concat.append(x)

            x = Concatenation(layers_concat)

            return x

    def Dense_net(self, input_x):
        x = conv_layer(input_x, filter=2 * self.filters, kernel=[7,7], stride=2, layer_name=&#39;conv0&#39;)
        # x = Max_Pooling(x, pool_size=[3,3], stride=2)


        &quot;&quot;&quot;
        for i in range(self.nb_blocks) :
            # 6 -&gt; 12 -&gt; 48
            x = self.dense_block(input_x=x, nb_layers=4, layer_name=&#39;dense_&#39;+str(i))
            x = self.transition_layer(x, scope=&#39;trans_&#39;+str(i))
        &quot;&quot;&quot;




        x = self.dense_block(input_x=x, nb_layers=6, layer_name=&#39;dense_1&#39;)
        x = self.transition_layer(x, scope=&#39;trans_1&#39;)

        x = self.dense_block(input_x=x, nb_layers=12, layer_name=&#39;dense_2&#39;)
        x = self.transition_layer(x, scope=&#39;trans_2&#39;)

        x = self.dense_block(input_x=x, nb_layers=48, layer_name=&#39;dense_3&#39;)
        x = self.transition_layer(x, scope=&#39;trans_3&#39;)

        x = self.dense_block(input_x=x, nb_layers=32, layer_name=&#39;dense_final&#39;)



        # 100 Layer
        x = Batch_Normalization(x, training=self.training, scope=&#39;linear_batch&#39;)
        x = Relu(x)
        x = Global_Average_Pooling(x)
        x = flatten(x)
        x = Linear(x)


        # x = tf.reshape(x, [-1, 10])
        return x



train_x, train_y, test_x, test_y = prepare_data()
train_x, test_x = color_preprocessing(train_x, test_x)

# image_size = 32, img_channels = 3, class_num = 10 in cifar10
x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])
label = tf.placeholder(tf.float32, shape=[None, class_num])

training_flag = tf.placeholder(tf.bool)


learning_rate = tf.placeholder(tf.float32, name=&#39;learning_rate&#39;)

logits = DenseNet(x=x, nb_blocks=nb_block, filters=growth_k, training=training_flag).model
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))

&quot;&quot;&quot;
l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])
optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=nesterov_momentum, use_nesterov=True)
train = optimizer.minimize(cost + l2_loss * weight_decay)
In paper, use MomentumOptimizer
init_learning_rate = 0.1
but, I&#39;ll use AdamOptimizer
&quot;&quot;&quot;

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)
train = optimizer.minimize(cost)


correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

saver = tf.train.Saver(tf.global_variables())

with tf.Session() as sess:
    ckpt = tf.train.get_checkpoint_state(&#39;./model&#39;)
    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
        saver.restore(sess, ckpt.model_checkpoint_path)
    else:
        sess.run(tf.global_variables_initializer())

    summary_writer = tf.summary.FileWriter(&#39;./logs&#39;, sess.graph)

    epoch_learning_rate = init_learning_rate
    for epoch in range(1, total_epochs + 1):
        if epoch == (total_epochs * 0.5) or epoch == (total_epochs * 0.75):
            epoch_learning_rate = epoch_learning_rate / 10

        pre_index = 0
        train_acc = 0.0
        train_loss = 0.0


        for step in range(1, iteration + 1):
            if pre_index+batch_size &lt; 50000 :
                batch_x = train_x[pre_index : pre_index+batch_size]
                batch_y = train_y[pre_index : pre_index+batch_size]
            else :
                batch_x = train_x[pre_index : ]
                batch_y = train_y[pre_index : ]

            batch_x = data_augmentation(batch_x)

            train_feed_dict = {
                x: batch_x,
                label: batch_y,
                learning_rate: epoch_learning_rate,
                training_flag : True
            }

            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)
            batch_acc = accuracy.eval(feed_dict=train_feed_dict)

            train_loss += batch_loss
            train_acc += batch_acc
            pre_index += batch_size

            if step == iteration :
                train_loss /= iteration # average loss
                train_acc /= iteration # average accuracy

                train_summary = tf.Summary(value=[tf.Summary.Value(tag=&#39;train_loss&#39;, simple_value=train_loss),
                                                  tf.Summary.Value(tag=&#39;train_accuracy&#39;, simple_value=train_acc)])

                test_acc, test_loss, test_summary = Evaluate(sess)

                summary_writer.add_summary(summary=train_summary, global_step=epoch)
                summary_writer.add_summary(summary=test_summary, global_step=epoch)
                summary_writer.flush()

                line = &quot;epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \n&quot; % (
                    epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)
                print(line)

                with open(&#39;logs.txt&#39;, &#39;a&#39;) as f :
                    f.write(line)



        saver.save(sess=sess, save_path=&#39;./model/dense.ckpt&#39;)
</code></pre>
<h2 id="一维代码"><a href="#一维代码" class="headerlink" title="一维代码"></a>一维代码</h2><p>由于我做的是一维数据识别，需要将代码改为一维，和修改数据读入。</p>
<p>修改下面部分即可改变网络结构，由于电脑显卡太差，原论文中的结构会因为显存不够而报错。</p>
<pre><code class="lang-python">def Dense_net(self, input_x):
        x = conv_layer(input_x, filter=2 * self.filters, kernel=[1, 7], stride=2, layer_name=&#39;conv0&#39;)#卷积
        x = Max_Pooling(x, pool_size=[1, 3], stride=2)#最大池化

        for i in range(self.nb_blocks):
            # 6 -&gt; 12 -&gt; 48
            x = self.dense_block(input_x=x, nb_layers=2, layer_name=&#39;dense_&#39; + str(i))
            x = self.transition_layer(x, scope=&#39;trans_&#39; + str(i))

        &quot;&quot;&quot;
        x = self.dense_block(input_x=x, nb_layers=6, layer_name=&#39;dense_1&#39;)
        x = self.transition_layer(x, scope=&#39;trans_1&#39;)

        x = self.dense_block(input_x=x, nb_layers=12, layer_name=&#39;dense_2&#39;)
        x = self.transition_layer(x, scope=&#39;trans_2&#39;)

        x = self.dense_block(input_x=x, nb_layers=48, layer_name=&#39;dense_3&#39;)
        x = self.transition_layer(x, scope=&#39;trans_3&#39;)
        &quot;&quot;&quot;  

        x = self.dense_block(input_x=x, nb_layers=4, layer_name=&#39;dense_final&#39;)

        # 100 Layer
        x = Batch_Normalization(x, training=self.training, scope=&#39;linear_batch&#39;)
        x = Relu(x)
        x = Global_Average_Pooling(x)
        x = flatten(x)
        x = Linear(x)

        # x = tf.reshape(x, [-1, 10])
        return x
</code></pre>
<h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><pre><code class="lang-python">import tensorflow as tf
from tflearn.layers.conv import global_avg_pool
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.contrib.layers import batch_norm, flatten
from tensorflow.contrib.framework import arg_scope
import numpy as np
import matplotlib.pyplot as plt


# Hyperparameter
growth_k = 12
nb_block = 1  # how many (dense block + Transition Layer) ?
init_learning_rate = 1e-3
epsilon = 1e-8  # AdamOptimizer epsilon
dropout_rate = 0.2

# Momentum Optimizer will use
nesterov_momentum = 0.9
weight_decay = 1e-4

# Label &amp; batch_size
class_num = 2


total_epochs = 400


DATA_SIZE = 1175
BATCH_SIZE =128

acc_print=[]
acc_printx=[]
acc_printy=[]

tf.reset_default_graph()
def conv_layer(input, filter, kernel, stride=1, layer_name=&quot;conv&quot;):
    with tf.name_scope(layer_name):
        network = tf.layers.conv2d(inputs=input, filters=filter, kernel_size=kernel, strides=stride, padding=&#39;SAME&#39;)
        return network


def Global_Average_Pooling(x, stride=1):
    &quot;&quot;&quot;
    width = np.shape(x)[1]
    height = np.shape(x)[2]
    pool_size = [width, height]
    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride) # The stride value does not matter
    It is global average pooling without tflearn
    &quot;&quot;&quot;

    return global_avg_pool(x, name=&#39;Global_avg_pooling&#39;)
    # But maybe you need to install h5py and curses or not


def Batch_Normalization(x, training, scope):
    with arg_scope([batch_norm],
                   scope=scope,
                   updates_collections=None,
                   decay=0.9,
                   center=True,
                   scale=True,
                   zero_debias_moving_mean=True):
        return tf.cond(training,
                       lambda: batch_norm(inputs=x, is_training=training, reuse=None),
                       lambda: batch_norm(inputs=x, is_training=training, reuse=True))


def Drop_out(x, rate, training):
    return tf.layers.dropout(inputs=x, rate=rate, training=training)


def Relu(x):
    return tf.nn.relu(x)


def Average_pooling(x, pool_size=[1, 2], stride=2, padding=&#39;VALID&#39;):
    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)


def Max_Pooling(x, pool_size=[1, 3], stride=2, padding=&#39;VALID&#39;):
    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)


def Concatenation(layers):
    return tf.concat(layers, axis=3)


def Linear(x):
    return tf.layers.dense(inputs=x, units=class_num, name=&#39;linear&#39;)


class DenseNet():
    def __init__(self, x, nb_blocks, filters, training):
        self.nb_blocks = nb_blocks
        self.filters = filters
        self.training = training
        self.model = self.Dense_net(x)

    def bottleneck_layer(self, x, scope):
        # print(x)
        with tf.name_scope(scope):
            x = Batch_Normalization(x, training=self.training, scope=scope + &#39;_batch1&#39;)
            x = Relu(x)
            x = conv_layer(x, filter=4 * self.filters, kernel=[1, 1], layer_name=scope + &#39;_conv1&#39;)
            x = Drop_out(x, rate=dropout_rate, training=self.training)

            x = Batch_Normalization(x, training=self.training, scope=scope + &#39;_batch2&#39;)
            x = Relu(x)
            x = conv_layer(x, filter=self.filters, kernel=[1, 3], layer_name=scope + &#39;_conv2&#39;)
            x = Drop_out(x, rate=dropout_rate, training=self.training)

            # print(x)

            return x

    def transition_layer(self, x, scope):
        with tf.name_scope(scope):
            x = Batch_Normalization(x, training=self.training, scope=scope + &#39;_batch1&#39;)
            x = Relu(x)
            x = conv_layer(x, filter=self.filters, kernel=[1,1], layer_name=scope+&#39;_conv1&#39;)

            # https://github.com/taki0112/Densenet-Tensorflow/issues/10

            #in_channel = x.shape[-1]
            #x = conv_layer(x, filter=in_channel * 1, kernel=[1, 1], layer_name=scope + &#39;_conv1&#39;)
            x = Drop_out(x, rate=dropout_rate, training=self.training)
            x = Average_pooling(x, pool_size=[1, 2], stride=2)

            return x

    def dense_block(self, input_x, nb_layers, layer_name):
        with tf.name_scope(layer_name):
            layers_concat = list()
            layers_concat.append(input_x)

            x = self.bottleneck_layer(input_x, scope=layer_name + &#39;_bottleN_&#39; + str(0))

            layers_concat.append(x)

            for i in range(nb_layers - 1):
                x = Concatenation(layers_concat)
                x = self.bottleneck_layer(x, scope=layer_name + &#39;_bottleN_&#39; + str(i + 1))
                layers_concat.append(x)

            x = Concatenation(layers_concat)

            return x

    def Dense_net(self, input_x):
        x = conv_layer(input_x, filter=2 * self.filters, kernel=[1, 7], stride=2, layer_name=&#39;conv0&#39;)#卷积
        x = Max_Pooling(x, pool_size=[1, 3], stride=2)#最大池化

        for i in range(self.nb_blocks):
            # 6 -&gt; 12 -&gt; 48
            x = self.dense_block(input_x=x, nb_layers=2, layer_name=&#39;dense_&#39; + str(i))
            x = self.transition_layer(x, scope=&#39;trans_&#39; + str(i))

        &quot;&quot;&quot;
        x = self.dense_block(input_x=x, nb_layers=6, layer_name=&#39;dense_1&#39;)
        x = self.transition_layer(x, scope=&#39;trans_1&#39;)

        x = self.dense_block(input_x=x, nb_layers=12, layer_name=&#39;dense_2&#39;)
        x = self.transition_layer(x, scope=&#39;trans_2&#39;)

        x = self.dense_block(input_x=x, nb_layers=48, layer_name=&#39;dense_3&#39;)
        x = self.transition_layer(x, scope=&#39;trans_3&#39;)
        &quot;&quot;&quot;  

        x = self.dense_block(input_x=x, nb_layers=4, layer_name=&#39;dense_final&#39;)

        # 100 Layer
        x = Batch_Normalization(x, training=self.training, scope=&#39;linear_batch&#39;)
        x = Relu(x)
        x = Global_Average_Pooling(x)
        x = flatten(x)
        x = Linear(x)

        # x = tf.reshape(x, [-1, 10])
        return x


def convert_to_one_hot(Y, C):       #转为1位热码编码
    Y = np.eye(C)[Y.reshape(-1)].T
    return Y

string0=np.loadtxt(&#39;4.30+5.1.txt&#39;,dtype=np.float32)
trainy=string0[:,0].reshape(-1,1).T  #1行
trainx=string0[:,1:].reshape(1175,-1)
trainy= convert_to_one_hot(trainy.astype(int), 2).T



x = tf.placeholder(tf.float32, shape=[None, 500])
batch_images = tf.reshape(x, [-1,1,500,1])

label = tf.placeholder(tf.float32, shape=[None, 2])

training_flag = tf.placeholder(tf.bool)

learning_rate = tf.placeholder(tf.float32, name=&#39;learning_rate&#39;)

logits = DenseNet(x=batch_images, nb_blocks=nb_block, filters=growth_k, training=training_flag).model
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))

&quot;&quot;&quot;
l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])
optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=nesterov_momentum, use_nesterov=True)
train = optimizer.minimize(cost + l2_loss * weight_decay)
In paper, use MomentumOptimizer
init_learning_rate = 0.1
but, I&#39;ll use AdamOptimizer
&quot;&quot;&quot;

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)
train = optimizer.minimize(cost)

correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))

tf.summary.scalar(&#39;loss&#39;, cost)
tf.summary.scalar(&#39;accuracy&#39;, accuracy)

saver = tf.train.Saver(tf.global_variables())

with tf.Session() as sess:
    ckpt = tf.train.get_checkpoint_state(&#39;./model&#39;)
    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):        #寻找模型路径
        saver.restore(sess, ckpt.model_checkpoint_path)
    else:
        sess.run(tf.global_variables_initializer())

    merged = tf.summary.merge_all()         #合并默认图中收集的所有摘要。
    writer = tf.summary.FileWriter(&#39;./logs&#39;, sess.graph)    #指定一个文件用来保存图

    global_step = 0
    epoch_learning_rate = init_learning_rate
    i=1
    for epoch in range(total_epochs):
        if epoch == (total_epochs * 0.5) or epoch == (total_epochs * 0.75):
            epoch_learning_rate = epoch_learning_rate / 10

        total_batch =10  #训练总次数

        for step in range(total_batch):
            start = (step * BATCH_SIZE) % DATA_SIZE
            end = min(start + BATCH_SIZE, DATA_SIZE)

            # 每次选取batch_size个样本进行训练
            # _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: trainx[start: end], y_: trainy[start: end]})
            batch_x =trainx[start: end]
            batch_y =trainy[start: end]

            train_feed_dict = {
                    x: batch_x,
                    label: batch_y,
                    learning_rate: epoch_learning_rate,
                    training_flag: True
                    }

            _, loss = sess.run([train, cost], feed_dict=train_feed_dict)

        if epoch % 10 == 0:
            global_step += 100
            train_summary, train_accuracy = sess.run( [merged,accuracy], feed_dict=train_feed_dict)
            #t_accuracy = sess.run(accuracy,feed_dict={x:batch_x,label:batch_y})
            print(&quot;Step:&quot;, epoch, &quot;Loss:&quot;, loss, &quot;Training accuracy:&quot;, train_accuracy)
            writer.add_summary(train_summary, global_step=epoch)
            acc_print.append(train_accuracy)
            acc_printy.append(loss)
            acc_printx.append(i)
            i+=1



        test_feed_dict = {
            x: trainx[start:end],
            label: trainy[start:end],
            learning_rate: epoch_learning_rate,
            training_flag: False
        }

        #accuracy_rates = sess.run(accuracy, feed_dict=test_feed_dict)
        #print(&#39;Epoch:&#39;, &#39;%04d&#39; % (epoch + 1), &#39;/ Accuracy =&#39;, accuracy_rates)
        # writer.add_summary(test_summary, global_step=epoch)
    plt.title(&quot;trend of accuracy&quot;)
    plt.plot(acc_printx,acc_print,color=&#39;r&#39;)
    plt.plot(acc_printx,acc_printy,color=&#39;cyan&#39;)
    plt.show()
    saver.save(sess=sess, save_path=&#39;./model/dense.ckpt&#39;)
</code></pre>
<h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2020/02/09/1fbO5n.png" alt="准确率和loss值" title>
                </div>
                <div class="image-caption">准确率和loss值</div>
            </figure>
<p>不得不说还是很强大的。</p>
<p><strong>参考连接</strong></p>
<p> <a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37189203</a> </p>
<p> <a href="https://www.cnblogs.com/skyfsm/p/8451834.html" target="_blank" rel="noopener">https://www.cnblogs.com/skyfsm/p/8451834.html</a> </p>
<p> <a href="https://github.com/taki0112/Densenet-Tensorflow" target="_blank" rel="noopener">https://github.com/taki0112/Densenet-Tensorflow</a> </p>
<div><strong>🚩推荐阅读</strong><ul><li><a href="https://1187100546.github.io/2020/01/14/lenet-5/">Lenet-5卷积神经网络结构</a></li><li><a href="http://univeryinli.coding.me/2019/07/03/BERT的前世今生/">BERT的前世今生</a></li><li><a href="http://univeryinli.github.io/2019/07/03/BERT的前世今生/">BERT的前世今生</a></li></ul></div>
        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2020-02-09T07:49:23.481Z" itemprop="dateUpdated">2020-02-09 15:49:23</time>
</span><br>


        
        文章发布地址：<a href="/2020/02/09/densenet/" target="_blank" rel="external">https://1187100546.github.io/2020/02/09/densenet/</a>
        
    </div>
    
    <footer>
        <a href="https://1187100546.github.io">
            <img src="/img/avatar.jpg" alt="huahua">
            huahua
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DenseNet/">DenseNet</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://1187100546.github.io/2020/02/09/densenet/&title=《DenseNet神经网络结构》 — 花花的blog&pic=https://1187100546.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://1187100546.github.io/2020/02/09/densenet/&title=《DenseNet神经网络结构》 — 花花的blog&source=CVPR 2017最佳论文DenseNet，论文中提出的DenseNet（Dense Convolutional Network） 。" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://1187100546.github.io/2020/02/09/densenet/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《DenseNet神经网络结构》 — 花花的blog&url=https://1187100546.github.io/2020/02/09/densenet/&via=https://1187100546.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://1187100546.github.io/2020/02/09/densenet/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2020/01/14/lenet-5/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Lenet-5卷积神经网络结构</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment v" id="vcomments"></div>
    <!-- <div class="comment" id="comment"></div> -->
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script>
    <!-- <script src="//t1.aixinxi.net/o_1c3n4pim01nl3jg91b6l1kjtkvsa.js"></script> -->
    <!-- <script src="/js/Valine.min.js"></script> -->
    <!-- <script src="https://cdnjs.cat.net/ajax/libs/jquery/3.2.1/jquery.min.js"></script> -->
    <script src="//cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            av: AV,
            // el: '#comments',
            el: '#vcomments',
            emoticon_url: 'https://abelsu7.top/alu', //表情图片网址
            emoticon_list: ["赞一个.png","坐等.png","长草.png","阴暗.png","邪恶.png","小眼睛.png","想一想.png","献黄瓜.png","献花.png","喜极而泣.png","无语.png","无所谓.png","无奈.png","投降.png","深思.png","期待.png","狂汗.png","蜡烛.png","看不见.png","惊喜.png","击掌.png","欢呼.png","得意.png","不出所料.png","观察.png"],//表情图片文件名
            // notify: 'false' == 'false',
            // verify: 'false' == 'false',
            // notify: 'false',
            // verify: 'false',
            notify: false,
            verify: false,
            appId: "xOsugK8HFYdHC2hNGosWRjdn-gzGzoHsz",
            appKey: "Azvn66MvIyLs9e2BrEzuWDcC",
            avatar: "mp",
            placeholder: "Write a comment",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->











</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        感谢支持！
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechatpay.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechatpay.png" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-item switch">切换</span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


            <p>
                
                    <span>
                        <a href="/atom.xml" target="_blank" class="rss" title="rss">
                            <i class="icon icon-lg icon-rss"></i>
                        </a>
                    </span>
                    
                        <span>
                            This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
                        </span>
            </p>
    </div>
    <div class="bottom">
        <p>
            <span>
                huahua &copy;
                    
                        2018 -
                            
                                2020
            </span>
            <span>
                
                        Power by
                        <a href="http://hexo.io/" target="_blank">Hexo</a> Theme
                        <a href="https://github.com/abelsu7/hexo-theme-indigo-plus" target="_blank">indigo plus</a>
                        <p>Hosted by <a href="https://pages.github.com" target="_blank" style="font-weight: bold">Github Pages</a></p>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>
<a href="javascript:;" id="gobottom" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-comments"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://1187100546.github.io/2020/02/09/densenet/&title=《DenseNet神经网络结构》 — 花花的blog&pic=https://1187100546.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://1187100546.github.io/2020/02/09/densenet/&title=《DenseNet神经网络结构》 — 花花的blog&source=CVPR 2017最佳论文DenseNet，论文中提出的DenseNet（Dense Convolutional Network） 。" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://1187100546.github.io/2020/02/09/densenet/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《DenseNet神经网络结构》 — 花花的blog&url=https://1187100546.github.io/2020/02/09/densenet/&via=https://1187100546.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://1187100546.github.io/2020/02/09/densenet/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACGklEQVR42u3aW47DMAgF0O5/05kFdJJeIK0U+/grqvLwcSWEMa9XPI63cXbP2f1nT12/5+aBgYHxWMZxOd7vyX854+XLlMwNAwNjH0YSQCfX15NI3vNhzhgYGBhxqM0nXV0ODAwMjLsY1Q1w8mYMDAyMfBObT2vytq/vxTEwMB7IqBbrf3n9lfMNDAyMRzGO4ugdc14/Ww3Z/3wFAwNjaUYe4KqFs1462FsyDAyMHRi9CV2H1GRCeSkt+QUDA2NVRi/MTZ6tlvCiwwkMDIwNGL2DzEmo7R2Unn4FAwNjaUa1WN9LFufb2qh1AwMDY1FGWTxI/qrgPOhjYGCszaimgBPeXYW56J/BwMBYlJED8qDcSyJ778HAwFiVUfANGi/ybWpvaTAwMHZg3BU0eyX+anL5YV0xMDA2YEwSteSYMymoVVs9MDAw1mYkUStvwuilejkjuh8DA2M5xrypq7cQOazQbIGBgbEoo5p+VZu9emG3WsjDwMDYgZGfc+ZFt+r2ePIUBgbGbox50a3c39E6bMDAwNiTMa/YTdLKfA6nT2FgYCzKOIojL5lNjgGqoR8DA2NtxqToX51u7zjz5lYzDAyMxzJ6pa4kcFcXKAdjYGDsyegFvsmW9Ud7cQwMjI0ZOaZX7h/1iWBgYGzPmKSDk/Lchy9iYGBswEg2sXkhrNdm0TsExcDA2IExb7C4ThbzQlu1AIeBgbEB4w+F+0HPCECzUAAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.jsdelivr.net/npm/node-waves@0.7.6/src/js/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
<!-- <script async src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script> -->
<!-- <script async src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script> -->




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<script src="/js/prism.min.js?v=1.7.2"></script>
<script src="/js/prism-vim.min.js?v=1.7.2"></script>
</body>
</html>
